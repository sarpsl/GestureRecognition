{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYBFSxzfSm0"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <font color='blue'>Index</font>\n",
        "- <font color='blue'>1. Initial Steps</font>\n",
        "\t- <font color='blue'>1.1 Imports</font>\n",
        "\t- <font color='blue'>1.2 Random Seeds</font>\n",
        "\t- <font color='blue'>1.3 Google Drive Connect</font>\n",
        "\t- <font color='blue'>1.4 Setting Paths</font>\n",
        "\t- <font color='blue'>1.5 Defining Generator</font>\n",
        "\t- <font color='blue'>1.6 Listing File Counts</font>\n",
        "- <font color='blue'>2. Models</font>\n",
        "\t- <font color='blue'>2.1 Model 1 - </font>\n",
        "\t\t- <font color='blue'>2.1.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.1.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.1.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.1.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.2 Model 2 - </font>\n",
        "\t\t- <font color='blue'>2.2.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.2.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.2.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.2.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.3 Model 3 - </font>\n",
        "\t\t- <font color='blue'>2.3.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.3.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.3.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.3.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.4 Model 4 - </font>\n",
        "\t\t- <font color='blue'>2.4.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.4.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.4.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.4.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.5 Model 5 - </font>\n",
        "\t\t- <font color='blue'>2.5.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.5.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.5.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.5.4 Next Actions</font>\n",
        "\t- <font color='blue'>2.6 Model 6 - </font>\n",
        "\t\t- <font color='blue'>2.6.1 Design, Compile & Summary</font>\n",
        "\t\t- <font color='blue'>2.6.2 Model Training</font>\n",
        "\t\t- <font color='blue'>2.6.3 Accuracy & Remarks</font>\n",
        "\t\t- <font color='blue'>2.6.4 Next Actions</font>\n",
        "- <font color='blue'>3. Conclusion & Final Selection</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>1. Initial Steps</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.1 Imports</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yAotwUfcfSm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import imageio.v2 as imageio\n",
        "from imageio.v2 import imread\n",
        "from skimage.transform import resize\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.applications import mobilenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.2 Random Seeds</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D24Jv6apfSm3"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aiewGHh_fSm3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGLD34vMfSm3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.3 Google Drive Connect</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkaxqxbDfSm3",
        "outputId": "0ec2cb13-97be-49ee-ce4f-dd6caea1beb1"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.4 Setting Paths</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HG7Zs-L-g7LF"
      },
      "outputs": [],
      "source": [
        "# train_doc = np.random.permutation(open('/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/train.csv').readlines())\n",
        "# val_doc = np.random.permutation(open('/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/val.csv').readlines())\n",
        "\n",
        "train_doc = np.random.permutation(open('G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\val.csv').readlines())\n",
        "\n",
        "batch_size = 64 #experiment with the batch size 16, 32, 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.5 Defining Generator</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtMUnfmfSm4"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "36MY7krNfSm4"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size, img_idx, shape_h, shape_w):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    # img_idx = [1,4,7,10,13,16,19,22,25,28] #create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = int(len(t)/batch_size) # calculate the number of batches\n",
        "        # left over batches which should be handled separately\n",
        "        leftover_batches = len(t) - num_batches * batch_size\n",
        "\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size, len(img_idx), shape_h, shape_w, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    image = resize(image, (shape_h, shape_w))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if leftover_batches != 0:\n",
        "            for batch in range(num_batches):\n",
        "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "                batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3))\n",
        "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
        "                batch_labels = np.zeros((batch_size,5))\n",
        "                for folder in range(batch_size): # iterate over the batch_size\n",
        "                    imgs = os.listdir(source_path +'/'+t[batch * batch_size + folder].split(';')[0])\n",
        "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "\n",
        "                        image = imageio.imread(source_path +'/'+t[batch * batch_size + folder].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
        "                        image = resize(image, (shape_h,shape_w))\n",
        "\n",
        "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                    #Fill the one hot encoding stuff where we maintain the label\n",
        "                    batch_labels[folder, int(t[batch * batch_size + folder].split(';')[2])] = 1\n",
        "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUEXzkB6fSm4"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>1.6 Listing File Counts</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsV9JJxIfSm4",
        "outputId": "56372afe-17a5-4d91-c759-b4a3f9936396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 15\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "# train_path = '/notebooks/storage/Final_data/Collated_training/train'\n",
        "# val_path = '/notebooks/storage/Final_data/Collated_training/val'\n",
        "# train_path = '/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/train'\n",
        "# val_path = '/content/gdrive/My Drive/Personal/upGrad/MS/Recurrent Neural Networks/Project_data/val'\n",
        "train_path = 'G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\train'\n",
        "val_path = 'G:\\\\My Drive\\\\Personal\\\\upGrad\\\\MS\\\\Recurrent Neural Networks\\\\Project_data\\\\val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 15 # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i26PWoagGCPz",
        "outputId": "b6458f6e-2bc1-4f7f-9049-2cb211739d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 1\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "shape_h = 50\n",
        "shape_w = 50\n",
        "batch_size = 1\n",
        "test_gen = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "d = next(test_gen)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "qxU9V0cROUeS",
        "outputId": "8c254130-0fc8-4f4f-9b50-422b973391a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1c52d392d90>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "plt.imshow(d[0,9,:,:,:])\n",
        "# axes[1].imshow(d[3,15,:,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>2. Models</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.1 Model 1 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYgj_BpfSm5",
        "outputId": "7e357750-e20e-43a3-b699-e9d7ae30425c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_24 (Conv3D)          (None, 10, 50, 50, 16)    1312      \n",
            "                                                                 \n",
            " max_pooling3d_24 (MaxPooli  (None, 5, 25, 25, 16)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_24 (Ba  (None, 5, 25, 25, 16)     64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_25 (Conv3D)          (None, 5, 25, 25, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_25 (MaxPooli  (None, 2, 12, 12, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_25 (Ba  (None, 2, 12, 12, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_26 (Conv3D)          (None, 2, 12, 12, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_26 (MaxPooli  (None, 1, 6, 6, 64)       0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_26 (Ba  (None, 1, 6, 6, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               295040    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 366661 (1.40 MB)\n",
            "Trainable params: 366437 (1.40 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 50\n",
        "shape_w = 50\n",
        "batch_size = 1\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ut5YMGEfSm6"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAukElrPfSm6",
        "outputId": "bde51969-2835-49ec-85f7-94d5ab8c6502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\2741796141.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 1.8976 - categorical_accuracy: 0.3952Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 1\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-1.89764-0.39517-2.16167-0.49000.h5\n",
            "663/663 [==============================] - 168s 249ms/step - loss: 1.8976 - categorical_accuracy: 0.3952 - val_loss: 2.1617 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 1.0559 - categorical_accuracy: 0.5958\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-1.05591-0.59578-1.08228-0.67000.h5\n",
            "663/663 [==============================] - 181s 274ms/step - loss: 1.0559 - categorical_accuracy: 0.5958 - val_loss: 1.0823 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.6762 - categorical_accuracy: 0.7602\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-0.67621-0.76018-0.92267-0.74000.h5\n",
            "663/663 [==============================] - 194s 292ms/step - loss: 0.6762 - categorical_accuracy: 0.7602 - val_loss: 0.9227 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.4351 - categorical_accuracy: 0.8296\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-0.43515-0.82956-1.05031-0.67000.h5\n",
            "663/663 [==============================] - 177s 268ms/step - loss: 0.4351 - categorical_accuracy: 0.8296 - val_loss: 1.0503 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.3042 - categorical_accuracy: 0.8793\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-0.30422-0.87934-1.45448-0.68000.h5\n",
            "663/663 [==============================] - 186s 281ms/step - loss: 0.3042 - categorical_accuracy: 0.8793 - val_loss: 1.4545 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2972 - categorical_accuracy: 0.8914\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-0.29718-0.89140-1.77237-0.65000.h5\n",
            "663/663 [==============================] - 174s 263ms/step - loss: 0.2972 - categorical_accuracy: 0.8914 - val_loss: 1.7724 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2699 - categorical_accuracy: 0.9186\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-0.26995-0.91855-0.92186-0.79000.h5\n",
            "663/663 [==============================] - 171s 258ms/step - loss: 0.2699 - categorical_accuracy: 0.9186 - val_loss: 0.9219 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1083 - categorical_accuracy: 0.9623\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-0.10829-0.96229-1.34807-0.76000.h5\n",
            "663/663 [==============================] - 157s 237ms/step - loss: 0.1083 - categorical_accuracy: 0.9623 - val_loss: 1.3481 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1660 - categorical_accuracy: 0.9563\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-0.16597-0.95626-2.78983-0.65000.h5\n",
            "663/663 [==============================] - 148s 223ms/step - loss: 0.1660 - categorical_accuracy: 0.9563 - val_loss: 2.7898 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.2557 - categorical_accuracy: 0.9065\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-0.25571-0.90649-0.91785-0.82000.h5\n",
            "663/663 [==============================] - 143s 216ms/step - loss: 0.2557 - categorical_accuracy: 0.9065 - val_loss: 0.9179 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0689 - categorical_accuracy: 0.9759\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-0.06892-0.97587-1.79006-0.75000.h5\n",
            "663/663 [==============================] - 140s 211ms/step - loss: 0.0689 - categorical_accuracy: 0.9759 - val_loss: 1.7901 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.1075 - categorical_accuracy: 0.9578\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-0.10747-0.95777-1.31524-0.81000.h5\n",
            "663/663 [==============================] - 141s 213ms/step - loss: 0.1075 - categorical_accuracy: 0.9578 - val_loss: 1.3152 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0290 - categorical_accuracy: 0.9894\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-0.02900-0.98944-1.34345-0.82000.h5\n",
            "663/663 [==============================] - 137s 207ms/step - loss: 0.0290 - categorical_accuracy: 0.9894 - val_loss: 1.3434 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0119 - categorical_accuracy: 0.9970\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-0.01188-0.99698-1.49615-0.83000.h5\n",
            "663/663 [==============================] - 185s 279ms/step - loss: 0.0119 - categorical_accuracy: 0.9970 - val_loss: 1.4962 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "663/663 [==============================] - ETA: 0s - loss: 0.0020 - categorical_accuracy: 1.0000\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-0.00201-1.00000-1.21714-0.86000.h5\n",
            "663/663 [==============================] - 139s 210ms/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 1.2171 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1c529fd5910>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "    \n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.1.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.2 Model 2 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 100\n",
        "shape_w = 100\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, image_height, image_width, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHHcxwpjfSm6",
        "outputId": "88b3de61-0b84-47f1-9d1e-a9030a4b0ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_27 (Conv3D)          (None, 10, 100, 100, 16   1312      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_27 (MaxPooli  (None, 5, 50, 50, 16)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_27 (Ba  (None, 5, 50, 50, 16)     64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_28 (Conv3D)          (None, 5, 50, 50, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_28 (MaxPooli  (None, 2, 25, 25, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_28 (Ba  (None, 2, 25, 25, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_29 (Conv3D)          (None, 2, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_29 (MaxPooli  (None, 1, 12, 12, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_29 (Ba  (None, 1, 12, 12, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               1179776   \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1251397 (4.77 MB)\n",
            "Trainable params: 1251173 (4.77 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\4088393871.py:35: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.3931 - categorical_accuracy: 0.5551Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-1.39308-0.55506-43.50549-0.23438.h5\n",
            "21/21 [==============================] - 183s 9s/step - loss: 1.3931 - categorical_accuracy: 0.5551 - val_loss: 43.5055 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3861 - categorical_accuracy: 0.8661\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-0.38611-0.86607-24.60771-0.32031.h5\n",
            "21/21 [==============================] - 180s 9s/step - loss: 0.3861 - categorical_accuracy: 0.8661 - val_loss: 24.6077 - val_categorical_accuracy: 0.3203 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1460 - categorical_accuracy: 0.9524\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-0.14597-0.95238-17.63374-0.29688.h5\n",
            "21/21 [==============================] - 188s 9s/step - loss: 0.1460 - categorical_accuracy: 0.9524 - val_loss: 17.6337 - val_categorical_accuracy: 0.2969 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0480 - categorical_accuracy: 0.9911\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-0.04799-0.99107-9.74919-0.43750.h5\n",
            "21/21 [==============================] - 164s 8s/step - loss: 0.0480 - categorical_accuracy: 0.9911 - val_loss: 9.7492 - val_categorical_accuracy: 0.4375 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0183 - categorical_accuracy: 0.9985\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-0.01833-0.99851-9.06219-0.42188.h5\n",
            "21/21 [==============================] - 157s 8s/step - loss: 0.0183 - categorical_accuracy: 0.9985 - val_loss: 9.0622 - val_categorical_accuracy: 0.4219 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0072 - categorical_accuracy: 1.0000\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-0.00718-1.00000-3.83495-0.58594.h5\n",
            "21/21 [==============================] - 164s 8s/step - loss: 0.0072 - categorical_accuracy: 1.0000 - val_loss: 3.8349 - val_categorical_accuracy: 0.5859 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0033 - categorical_accuracy: 1.0000\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-0.00328-1.00000-2.23472-0.64844.h5\n",
            "21/21 [==============================] - 172s 9s/step - loss: 0.0033 - categorical_accuracy: 1.0000 - val_loss: 2.2347 - val_categorical_accuracy: 0.6484 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0028 - categorical_accuracy: 1.0000\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-0.00279-1.00000-1.79519-0.60156.h5\n",
            "21/21 [==============================] - 163s 8s/step - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 1.7952 - val_categorical_accuracy: 0.6016 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0020 - categorical_accuracy: 1.0000\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-0.00196-1.00000-1.01077-0.76562.h5\n",
            "21/21 [==============================] - 162s 8s/step - loss: 0.0020 - categorical_accuracy: 1.0000 - val_loss: 1.0108 - val_categorical_accuracy: 0.7656 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 9.7035e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-0.00097-1.00000-0.58248-0.85156.h5\n",
            "21/21 [==============================] - 166s 8s/step - loss: 9.7035e-04 - categorical_accuracy: 1.0000 - val_loss: 0.5825 - val_categorical_accuracy: 0.8516 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 7.1579e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-0.00072-1.00000-0.41947-0.86719.h5\n",
            "21/21 [==============================] - 180s 9s/step - loss: 7.1579e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4195 - val_categorical_accuracy: 0.8672 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 5.1993e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-0.00052-1.00000-0.45508-0.83594.h5\n",
            "21/21 [==============================] - 188s 9s/step - loss: 5.1993e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4551 - val_categorical_accuracy: 0.8359 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 5.6876e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-0.00057-1.00000-0.34936-0.87500.h5\n",
            "21/21 [==============================] - 177s 9s/step - loss: 5.6876e-04 - categorical_accuracy: 1.0000 - val_loss: 0.3494 - val_categorical_accuracy: 0.8750 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 4.8627e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-0.00049-1.00000-0.45296-0.86719.h5\n",
            "21/21 [==============================] - 167s 8s/step - loss: 4.8627e-04 - categorical_accuracy: 1.0000 - val_loss: 0.4530 - val_categorical_accuracy: 0.8672 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 3.6004e-04 - categorical_accuracy: 1.0000\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-0.00036-1.00000-0.25769-0.92969.h5\n",
            "21/21 [==============================] - 185s 9s/step - loss: 3.6004e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2577 - val_categorical_accuracy: 0.9297 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1c52a37c810>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.2.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the difference between Categorical Accuracy (1.0) and ValidationAccuracy (0.89) is high, we will introduce dropout to overcome overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.3 Model 3 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS2rAehy2NJO"
      },
      "source": [
        "#### <font color='blue'>2.3.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 100\n",
        "shape_w = 100\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', input_shape=(frames, shape_h, shape_w, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmR_XI-9nnaH",
        "outputId": "508807b0-b9e8-40fb-d59b-b4f9ad0f02a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_30 (Conv3D)          (None, 10, 100, 100, 16   1312      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_30 (MaxPooli  (None, 5, 50, 50, 16)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 5, 50, 50, 16)     0         \n",
            "                                                                 \n",
            " batch_normalization_30 (Ba  (None, 5, 50, 50, 16)     64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_31 (Conv3D)          (None, 5, 50, 50, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_31 (MaxPooli  (None, 2, 25, 25, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 2, 25, 25, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_31 (Ba  (None, 2, 25, 25, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_32 (Conv3D)          (None, 2, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_32 (MaxPooli  (None, 1, 12, 12, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 1, 12, 12, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_32 (Ba  (None, 1, 12, 12, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1251397 (4.77 MB)\n",
            "Trainable params: 1251173 (4.77 MB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\824533343.py:61: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.0993 - categorical_accuracy: 0.3155Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-2.09931-0.31548-15.87268-0.25000.h5\n",
            "21/21 [==============================] - 187s 9s/step - loss: 2.0993 - categorical_accuracy: 0.3155 - val_loss: 15.8727 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.3849 - categorical_accuracy: 0.4583\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-1.38489-0.45833-3.25232-0.25000.h5\n",
            "21/21 [==============================] - 195s 10s/step - loss: 1.3849 - categorical_accuracy: 0.4583 - val_loss: 3.2523 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1799 - categorical_accuracy: 0.5402\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-1.17987-0.54018-1.65134-0.39062.h5\n",
            "21/21 [==============================] - 165s 8s/step - loss: 1.1799 - categorical_accuracy: 0.5402 - val_loss: 1.6513 - val_categorical_accuracy: 0.3906 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0556 - categorical_accuracy: 0.5804\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-1.05560-0.58036-0.85376-0.69531.h5\n",
            "21/21 [==============================] - 166s 8s/step - loss: 1.0556 - categorical_accuracy: 0.5804 - val_loss: 0.8538 - val_categorical_accuracy: 0.6953 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.9595 - categorical_accuracy: 0.6265\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-0.95948-0.62649-0.97235-0.67188.h5\n",
            "21/21 [==============================] - 165s 8s/step - loss: 0.9595 - categorical_accuracy: 0.6265 - val_loss: 0.9723 - val_categorical_accuracy: 0.6719 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8100 - categorical_accuracy: 0.6577\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-0.81001-0.65774-1.72313-0.46875.h5\n",
            "21/21 [==============================] - 155s 8s/step - loss: 0.8100 - categorical_accuracy: 0.6577 - val_loss: 1.7231 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7799 - categorical_accuracy: 0.6741\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-0.77992-0.67411-2.88565-0.31250.h5\n",
            "21/21 [==============================] - 164s 8s/step - loss: 0.7799 - categorical_accuracy: 0.6741 - val_loss: 2.8856 - val_categorical_accuracy: 0.3125 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7229 - categorical_accuracy: 0.7188\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-0.72290-0.71875-2.23340-0.43750.h5\n",
            "21/21 [==============================] - 186s 9s/step - loss: 0.7229 - categorical_accuracy: 0.7188 - val_loss: 2.2334 - val_categorical_accuracy: 0.4375 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6520 - categorical_accuracy: 0.7321\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-0.65199-0.73214-2.68714-0.33594.h5\n",
            "21/21 [==============================] - 167s 8s/step - loss: 0.6520 - categorical_accuracy: 0.7321 - val_loss: 2.6871 - val_categorical_accuracy: 0.3359 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5747 - categorical_accuracy: 0.7708\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-0.57470-0.77083-3.02071-0.32031.h5\n",
            "21/21 [==============================] - 162s 8s/step - loss: 0.5747 - categorical_accuracy: 0.7708 - val_loss: 3.0207 - val_categorical_accuracy: 0.3203 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5181 - categorical_accuracy: 0.7976\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-0.51810-0.79762-3.15691-0.35938.h5\n",
            "21/21 [==============================] - 173s 9s/step - loss: 0.5181 - categorical_accuracy: 0.7976 - val_loss: 3.1569 - val_categorical_accuracy: 0.3594 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4617 - categorical_accuracy: 0.8155\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-0.46168-0.81548-3.54497-0.41406.h5\n",
            "21/21 [==============================] - 166s 8s/step - loss: 0.4617 - categorical_accuracy: 0.8155 - val_loss: 3.5450 - val_categorical_accuracy: 0.4141 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4831 - categorical_accuracy: 0.8051\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-0.48309-0.80506-4.65938-0.29688.h5\n",
            "21/21 [==============================] - 174s 9s/step - loss: 0.4831 - categorical_accuracy: 0.8051 - val_loss: 4.6594 - val_categorical_accuracy: 0.2969 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4246 - categorical_accuracy: 0.8259\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-0.42464-0.82589-4.48733-0.32031.h5\n",
            "21/21 [==============================] - 161s 8s/step - loss: 0.4246 - categorical_accuracy: 0.8259 - val_loss: 4.4873 - val_categorical_accuracy: 0.3203 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3966 - categorical_accuracy: 0.8542\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-0.39663-0.85417-5.06669-0.37500.h5\n",
            "21/21 [==============================] - 169s 8s/step - loss: 0.3966 - categorical_accuracy: 0.8542 - val_loss: 5.0667 - val_categorical_accuracy: 0.3750 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1c5518fa490>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.3.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lakRzPt0NA8l"
      },
      "source": [
        "Due to the low validation accuracy above,\n",
        "- batch_size = 64\n",
        "- changing the architecture to start with 32 going into 256\n",
        "- ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.4 Model 4 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 100\n",
        "shape_w = 100\n",
        "batch_size = 64\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', input_shape=(frames, shape_h, shape_w, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ-gaR6o69-8",
        "outputId": "c8edc076-ebb3-40fa-9023-188bf59e85f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_33 (Conv3D)          (None, 10, 100, 100, 32   2624      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " max_pooling3d_33 (MaxPooli  (None, 5, 50, 50, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 5, 50, 50, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_33 (Ba  (None, 5, 50, 50, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_34 (Conv3D)          (None, 5, 50, 50, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_34 (MaxPooli  (None, 2, 25, 25, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 2, 25, 25, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_34 (Ba  (None, 2, 25, 25, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_35 (Conv3D)          (None, 2, 25, 25, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_35 (MaxPooli  (None, 1, 12, 12, 128)    0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 1, 12, 12, 128)    0         \n",
            "                                                                 \n",
            " batch_normalization_35 (Ba  (None, 1, 12, 12, 128)    512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 18432)             0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 256)               4718848   \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5000325 (19.07 MB)\n",
            "Trainable params: 4999877 (19.07 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\1833668871.py:61: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.1916 - categorical_accuracy: 0.3267 Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 64\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-2.19160-0.32670-47.58380-0.28125.h5\n",
            "11/11 [==============================] - 277s 27s/step - loss: 2.1916 - categorical_accuracy: 0.3267 - val_loss: 47.5838 - val_categorical_accuracy: 0.2812 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2346 - categorical_accuracy: 0.5611 \n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-1.23461-0.56108-42.14281-0.21875.h5\n",
            "11/11 [==============================] - 256s 25s/step - loss: 1.2346 - categorical_accuracy: 0.5611 - val_loss: 42.1428 - val_categorical_accuracy: 0.2188 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9868 - categorical_accuracy: 0.6250 \n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-0.98680-0.62500-37.99257-0.17188.h5\n",
            "11/11 [==============================] - 260s 25s/step - loss: 0.9868 - categorical_accuracy: 0.6250 - val_loss: 37.9926 - val_categorical_accuracy: 0.1719 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7453 - categorical_accuracy: 0.7145 \n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-0.74532-0.71449-20.70171-0.22656.h5\n",
            "11/11 [==============================] - 269s 26s/step - loss: 0.7453 - categorical_accuracy: 0.7145 - val_loss: 20.7017 - val_categorical_accuracy: 0.2266 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6387 - categorical_accuracy: 0.7244 \n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-0.63869-0.72443-13.69952-0.25000.h5\n",
            "11/11 [==============================] - 263s 26s/step - loss: 0.6387 - categorical_accuracy: 0.7244 - val_loss: 13.6995 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5235 - categorical_accuracy: 0.7940 \n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-0.52351-0.79403-7.77263-0.35938.h5\n",
            "11/11 [==============================] - 256s 25s/step - loss: 0.5235 - categorical_accuracy: 0.7940 - val_loss: 7.7726 - val_categorical_accuracy: 0.3594 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5247 - categorical_accuracy: 0.8423 \n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-0.52471-0.84233-5.54805-0.46875.h5\n",
            "11/11 [==============================] - 220s 22s/step - loss: 0.5247 - categorical_accuracy: 0.8423 - val_loss: 5.5480 - val_categorical_accuracy: 0.4688 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4468 - categorical_accuracy: 0.8310 \n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-0.44681-0.83097-2.44967-0.55469.h5\n",
            "11/11 [==============================] - 231s 23s/step - loss: 0.4468 - categorical_accuracy: 0.8310 - val_loss: 2.4497 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3162 - categorical_accuracy: 0.8807 \n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-0.31619-0.88068-1.78674-0.60938.h5\n",
            "11/11 [==============================] - 258s 25s/step - loss: 0.3162 - categorical_accuracy: 0.8807 - val_loss: 1.7867 - val_categorical_accuracy: 0.6094 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3146 - categorical_accuracy: 0.8622 \n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-0.31462-0.86222-0.69066-0.79688.h5\n",
            "11/11 [==============================] - 253s 25s/step - loss: 0.3146 - categorical_accuracy: 0.8622 - val_loss: 0.6907 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2326 - categorical_accuracy: 0.9077 \n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-0.23256-0.90767-0.66232-0.87500.h5\n",
            "11/11 [==============================] - 263s 26s/step - loss: 0.2326 - categorical_accuracy: 0.9077 - val_loss: 0.6623 - val_categorical_accuracy: 0.8750 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2707 - categorical_accuracy: 0.9020 \n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-0.27067-0.90199-0.44528-0.86719.h5\n",
            "11/11 [==============================] - 259s 25s/step - loss: 0.2707 - categorical_accuracy: 0.9020 - val_loss: 0.4453 - val_categorical_accuracy: 0.8672 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1752 - categorical_accuracy: 0.9418 \n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-0.17520-0.94176-0.60756-0.81250.h5\n",
            "11/11 [==============================] - 257s 25s/step - loss: 0.1752 - categorical_accuracy: 0.9418 - val_loss: 0.6076 - val_categorical_accuracy: 0.8125 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2412 - categorical_accuracy: 0.9105 \n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-0.24117-0.91051-0.62651-0.78125.h5\n",
            "11/11 [==============================] - 262s 26s/step - loss: 0.2412 - categorical_accuracy: 0.9105 - val_loss: 0.6265 - val_categorical_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1528 - categorical_accuracy: 0.9403 \n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-0.15283-0.94034-1.06300-0.76562.h5\n",
            "11/11 [==============================] - 262s 26s/step - loss: 0.1528 - categorical_accuracy: 0.9403 - val_loss: 1.0630 - val_categorical_accuracy: 0.7656 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.4.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-H3yuTg8hy"
      },
      "source": [
        "Cropping the image to 50 x 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.5 Model 5 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 50\n",
        "shape_w = 50\n",
        "batch_size = 64\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', input_shape=(frames, shape_h, shape_w, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(64, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(128, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yjNjRE0Tg5hx",
        "outputId": "073e367e-a352-4326-b5fb-3a3864be1341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_36 (Conv3D)          (None, 10, 50, 50, 32)    2624      \n",
            "                                                                 \n",
            " max_pooling3d_36 (MaxPooli  (None, 5, 25, 25, 32)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 5, 25, 25, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_36 (Ba  (None, 5, 25, 25, 32)     128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_37 (Conv3D)          (None, 5, 25, 25, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_37 (MaxPooli  (None, 2, 12, 12, 64)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 2, 12, 12, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_37 (Ba  (None, 2, 12, 12, 64)     256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_38 (Conv3D)          (None, 2, 12, 12, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_38 (MaxPooli  (None, 1, 6, 6, 128)      0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 1, 6, 6, 128)      0         \n",
            "                                                                 \n",
            " batch_normalization_38 (Ba  (None, 1, 6, 6, 128)      512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 256)               1179904   \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1461381 (5.57 MB)\n",
            "Trainable params: 1460933 (5.57 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\2628521629.py:61: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.0928 - categorical_accuracy: 0.3068 Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 64\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-2.09285-0.30682-38.74179-0.18750.h5\n",
            "11/11 [==============================] - 258s 25s/step - loss: 2.0928 - categorical_accuracy: 0.3068 - val_loss: 38.7418 - val_categorical_accuracy: 0.1875 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.6745 - categorical_accuracy: 0.4162 \n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-1.67454-0.41619-22.17600-0.22656.h5\n",
            "11/11 [==============================] - 230s 23s/step - loss: 1.6745 - categorical_accuracy: 0.4162 - val_loss: 22.1760 - val_categorical_accuracy: 0.2266 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.3853 - categorical_accuracy: 0.4872 \n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-1.38534-0.48722-17.58845-0.23438.h5\n",
            "11/11 [==============================] - 220s 22s/step - loss: 1.3853 - categorical_accuracy: 0.4872 - val_loss: 17.5884 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.1781 - categorical_accuracy: 0.5597 \n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-1.17812-0.55966-11.75618-0.24219.h5\n",
            "11/11 [==============================] - 210s 21s/step - loss: 1.1781 - categorical_accuracy: 0.5597 - val_loss: 11.7562 - val_categorical_accuracy: 0.2422 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.1245 - categorical_accuracy: 0.5682 \n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-1.12449-0.56818-8.01785-0.23438.h5\n",
            "11/11 [==============================] - 214s 21s/step - loss: 1.1245 - categorical_accuracy: 0.5682 - val_loss: 8.0179 - val_categorical_accuracy: 0.2344 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8914 - categorical_accuracy: 0.6435 \n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-0.89138-0.64347-5.23512-0.25000.h5\n",
            "11/11 [==============================] - 224s 22s/step - loss: 0.8914 - categorical_accuracy: 0.6435 - val_loss: 5.2351 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7998 - categorical_accuracy: 0.6818 \n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-0.79985-0.68182-2.92916-0.37500.h5\n",
            "11/11 [==============================] - 192s 19s/step - loss: 0.7998 - categorical_accuracy: 0.6818 - val_loss: 2.9292 - val_categorical_accuracy: 0.3750 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8173 - categorical_accuracy: 0.6776 \n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-0.81732-0.67756-1.33098-0.55469.h5\n",
            "11/11 [==============================] - 218s 22s/step - loss: 0.8173 - categorical_accuracy: 0.6776 - val_loss: 1.3310 - val_categorical_accuracy: 0.5547 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6608 - categorical_accuracy: 0.7429 \n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-0.66076-0.74290-0.89843-0.67188.h5\n",
            "11/11 [==============================] - 215s 21s/step - loss: 0.6608 - categorical_accuracy: 0.7429 - val_loss: 0.8984 - val_categorical_accuracy: 0.6719 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6152 - categorical_accuracy: 0.7486 \n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-0.61524-0.74858-0.68071-0.78125.h5\n",
            "11/11 [==============================] - 226s 23s/step - loss: 0.6152 - categorical_accuracy: 0.7486 - val_loss: 0.6807 - val_categorical_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5413 - categorical_accuracy: 0.7912 \n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-0.54130-0.79119-0.55750-0.79688.h5\n",
            "11/11 [==============================] - 228s 23s/step - loss: 0.5413 - categorical_accuracy: 0.7912 - val_loss: 0.5575 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5103 - categorical_accuracy: 0.7969 \n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-0.51030-0.79688-0.51398-0.79688.h5\n",
            "11/11 [==============================] - 216s 21s/step - loss: 0.5103 - categorical_accuracy: 0.7969 - val_loss: 0.5140 - val_categorical_accuracy: 0.7969 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5049 - categorical_accuracy: 0.8097 \n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-0.50486-0.80966-0.52253-0.85938.h5\n",
            "11/11 [==============================] - 208s 21s/step - loss: 0.5049 - categorical_accuracy: 0.8097 - val_loss: 0.5225 - val_categorical_accuracy: 0.8594 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4425 - categorical_accuracy: 0.8253 \n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-0.44246-0.82528-0.70851-0.71094.h5\n",
            "11/11 [==============================] - 218s 22s/step - loss: 0.4425 - categorical_accuracy: 0.8253 - val_loss: 0.7085 - val_categorical_accuracy: 0.7109 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4088 - categorical_accuracy: 0.8423 \n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-0.40880-0.84233-0.67402-0.76562.h5\n",
            "11/11 [==============================] - 216s 22s/step - loss: 0.4088 - categorical_accuracy: 0.8423 - val_loss: 0.6740 - val_categorical_accuracy: 0.7656 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.5.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7GMAyyqknPR"
      },
      "source": [
        "As we got an error starting with 32 filters, reducing it to start with 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='blue'>2.6 Model 6 - </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.1 Design, Compile & Summary</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_idx = list(range(1,30,3))\n",
        "frames = len(img_idx)\n",
        "shape_h = 50\n",
        "shape_w = 50\n",
        "batch_size = 32\n",
        "num_classes = 5\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(8, (3,3,3), padding='same', input_shape=(frames, shape_h, shape_w, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(16, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(32, (3,3,3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "optimiser = \"adam\"\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.2 Model Training</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-SuLH3OykglP",
        "outputId": "d841db2f-80cf-40c0-9dfb-c0b30afcc2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_39 (Conv3D)          (None, 10, 50, 50, 8)     656       \n",
            "                                                                 \n",
            " max_pooling3d_39 (MaxPooli  (None, 5, 25, 25, 8)      0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 5, 25, 25, 8)      0         \n",
            "                                                                 \n",
            " batch_normalization_39 (Ba  (None, 5, 25, 25, 8)      32        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_40 (Conv3D)          (None, 5, 25, 25, 16)     3472      \n",
            "                                                                 \n",
            " max_pooling3d_40 (MaxPooli  (None, 2, 12, 12, 16)     0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 2, 12, 12, 16)     0         \n",
            "                                                                 \n",
            " batch_normalization_40 (Ba  (None, 2, 12, 12, 16)     64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv3d_41 (Conv3D)          (None, 2, 12, 12, 32)     13856     \n",
            "                                                                 \n",
            " max_pooling3d_41 (MaxPooli  (None, 1, 6, 6, 32)       0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 1, 6, 6, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_41 (Ba  (None, 1, 6, 6, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 64)                73792     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 92325 (360.64 KB)\n",
            "Trainable params: 92213 (360.21 KB)\n",
            "Non-trainable params: 112 (448.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\train ; batch size = 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASyed\\AppData\\Local\\Temp\\ipykernel_8196\\149713886.py:61: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.1624 - categorical_accuracy: 0.2485Source path =  G:\\My Drive\\Personal\\upGrad\\MS\\Recurrent Neural Networks\\Project_data\\val ; batch size = 32\n",
            "\n",
            "Epoch 1: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00001-2.16245-0.24851-6.90674-0.25000.h5\n",
            "21/21 [==============================] - 185s 9s/step - loss: 2.1624 - categorical_accuracy: 0.2485 - val_loss: 6.9067 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.9454 - categorical_accuracy: 0.2976\n",
            "Epoch 2: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00002-1.94536-0.29762-3.75097-0.25000.h5\n",
            "21/21 [==============================] - 163s 8s/step - loss: 1.9454 - categorical_accuracy: 0.2976 - val_loss: 3.7510 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.6242 - categorical_accuracy: 0.3631\n",
            "Epoch 3: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00003-1.62415-0.36310-1.42736-0.40625.h5\n",
            "21/21 [==============================] - 188s 9s/step - loss: 1.6242 - categorical_accuracy: 0.3631 - val_loss: 1.4274 - val_categorical_accuracy: 0.4062 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.4663 - categorical_accuracy: 0.3705\n",
            "Epoch 4: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00004-1.46634-0.37054-1.19069-0.57031.h5\n",
            "21/21 [==============================] - 193s 10s/step - loss: 1.4663 - categorical_accuracy: 0.3705 - val_loss: 1.1907 - val_categorical_accuracy: 0.5703 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.3967 - categorical_accuracy: 0.4107\n",
            "Epoch 5: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00005-1.39666-0.41071-1.30734-0.46094.h5\n",
            "21/21 [==============================] - 156s 8s/step - loss: 1.3967 - categorical_accuracy: 0.4107 - val_loss: 1.3073 - val_categorical_accuracy: 0.4609 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2994 - categorical_accuracy: 0.4449\n",
            "Epoch 6: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00006-1.29940-0.44494-1.65617-0.32812.h5\n",
            "21/21 [==============================] - 142s 7s/step - loss: 1.2994 - categorical_accuracy: 0.4449 - val_loss: 1.6562 - val_categorical_accuracy: 0.3281 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2799 - categorical_accuracy: 0.4583\n",
            "Epoch 7: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00007-1.27988-0.45833-2.02094-0.30469.h5\n",
            "21/21 [==============================] - 147s 7s/step - loss: 1.2799 - categorical_accuracy: 0.4583 - val_loss: 2.0209 - val_categorical_accuracy: 0.3047 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2279 - categorical_accuracy: 0.4881\n",
            "Epoch 8: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00008-1.22788-0.48810-2.47819-0.30469.h5\n",
            "21/21 [==============================] - 179s 9s/step - loss: 1.2279 - categorical_accuracy: 0.4881 - val_loss: 2.4782 - val_categorical_accuracy: 0.3047 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2230 - categorical_accuracy: 0.4628\n",
            "Epoch 9: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00009-1.22299-0.46280-2.72844-0.28906.h5\n",
            "21/21 [==============================] - 186s 9s/step - loss: 1.2230 - categorical_accuracy: 0.4628 - val_loss: 2.7284 - val_categorical_accuracy: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2117 - categorical_accuracy: 0.5060\n",
            "Epoch 10: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00010-1.21171-0.50595-2.91449-0.32031.h5\n",
            "21/21 [==============================] - 188s 9s/step - loss: 1.2117 - categorical_accuracy: 0.5060 - val_loss: 2.9145 - val_categorical_accuracy: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1696 - categorical_accuracy: 0.4777\n",
            "Epoch 11: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00011-1.16955-0.47768-3.29912-0.31250.h5\n",
            "21/21 [==============================] - 181s 9s/step - loss: 1.1696 - categorical_accuracy: 0.4777 - val_loss: 3.2991 - val_categorical_accuracy: 0.3125 - lr: 2.5000e-04\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1591 - categorical_accuracy: 0.5253\n",
            "Epoch 12: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00012-1.15909-0.52530-3.39605-0.28125.h5\n",
            "21/21 [==============================] - 187s 9s/step - loss: 1.1591 - categorical_accuracy: 0.5253 - val_loss: 3.3960 - val_categorical_accuracy: 0.2812 - lr: 2.5000e-04\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1456 - categorical_accuracy: 0.5580\n",
            "Epoch 13: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00013-1.14564-0.55804-3.75350-0.22656.h5\n",
            "21/21 [==============================] - 188s 9s/step - loss: 1.1456 - categorical_accuracy: 0.5580 - val_loss: 3.7535 - val_categorical_accuracy: 0.2266 - lr: 2.5000e-04\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1207 - categorical_accuracy: 0.5089\n",
            "Epoch 14: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00014-1.12069-0.50893-3.51539-0.19531.h5\n",
            "21/21 [==============================] - 191s 10s/step - loss: 1.1207 - categorical_accuracy: 0.5089 - val_loss: 3.5154 - val_categorical_accuracy: 0.1953 - lr: 1.2500e-04\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0942 - categorical_accuracy: 0.5551\n",
            "Epoch 15: saving model to C:\\Temp\\RNN\\model_init_2024-03-0422_32_28.038415\\model-00015-1.09425-0.55506-3.65678-0.22656.h5\n",
            "21/21 [==============================] - 189s 9s/step - loss: 1.0942 - categorical_accuracy: 0.5551 - val_loss: 3.6568 - val_categorical_accuracy: 0.2266 - lr: 1.2500e-04\n"
          ]
        }
      ],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "val_generator = generator(val_path, val_doc, batch_size, img_idx, shape_h, shape_w)\n",
        "\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = 'C:\\\\Temp\\\\RNN\\\\' + model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.3 Accuracy & Remarks</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <font color='blue'>2.6.4 Next Actions</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font color='blue'>3. Conclusion & Final Selection</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
